<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-01-26T22:33:15+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">gautham | capemox</title><subtitle>capemox&apos;s site!
</subtitle><author><name>capemox</name></author><entry><title type="html">N-grams: The dumbest language models</title><link href="http://localhost:4000/ngrams.html" rel="alternate" type="text/html" title="N-grams: The dumbest language models" /><published>2025-01-26T00:00:00+05:30</published><updated>2025-01-26T00:00:00+05:30</updated><id>http://localhost:4000/ngrams</id><content type="html" xml:base="http://localhost:4000/ngrams.html"><![CDATA[<p>N-grams are some of the simplest language models out there, and are pretty easy to understand (compared to LLMs at least). In this article, I’ll try giving you an inuitive understanding of N-gram LMs, while also explaining perplexity, an evaluation measure for such LMs, and also the inspiration for <a href="https://www.perplexity.ai">perplexity.ai</a>! Stay tuned for its release!</p>

<hr />]]></content><author><name>capemox</name></author><summary type="html"><![CDATA[N-grams are some of the simplest language models out there, and are pretty easy to understand (compared to LLMs at least). In this article, I’ll try giving you an inuitive understanding of N-gram LMs, while also explaining perplexity, an evaluation measure for such LMs, and also the inspiration for perplexity.ai! Stay tuned for its release!]]></summary></entry></feed>