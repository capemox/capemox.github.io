<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- for mathjax support -->
    <!--  -->
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: {
              equationNumbers: {
                autoNumber: "AMS"
              }
            },
            tex2jax: {
            inlineMath: [ ['$', '$'] ],
            displayMath: [ ['$$', '$$'] ],
            processEscapes: true,
          }
        });
      </script>
      <!-- <script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!--  -->

  <title>N-grams: The dumbest language models</title>

  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="gautham | capemox" /><link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
  <link rel="stylesheet" href="/assets/css/main.css" />
	<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.css" integrity="sha384-OH8qNTHoMMVNVcKdKewlipV4SErXqccxxlg6HC9Cwjr5oZu2AdBej1TndeCirael" crossorigin="anonymous"> -->
  <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous"> -->
</head>
<body a="dark">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/" style="font-size: xx-large;">⟻</a><article>
  <p class="post-meta">
    <time datetime="2025-01-26 00:00:00 +0530">2025-01-26</time>
  </p>
  
  <h1>N-grams: The dumbest language models</h1>

  <p>N-grams are some of the simplest language models out there, and are pretty easy to understand (compared to LLMs at least). In this article, I’ll try giving you an inuitive understanding of N-gram LMs, while also explaining perplexity, an evaluation measure for such LMs, and also the inspiration for <a href="https://www.perplexity.ai">perplexity.ai</a>! Stay tuned for its release!</p>

<hr />
<!-- ![n-gram-1](/media/n-grams/n_gram_1.webp) -->

<h2 id="back-to-basics-counting">Back to Basics: Counting</h2>

<p>Let’s say you want to know what’s the weather like tomorrow. You <em>can</em> use state-of-the-art atmospheric models that solve numerous partial differential equations on big mainframe computers. But you can also, you know, just predict that it will rain tomorrow, because it rained today and yesterday.</p>

<p>This more or less highlights the difference between how LLMs (or the transformer model behind LLMs) work compared to N-grams. While the analogy isn’t <em>exactly</em> correct, it highlights just how simple N-grams are to grasp and build, and it also tells you that they’re probably not very powerful.</p>

<p><img src="https://imgs.xkcd.com/comics/10_day_forecast_2x.png" alt="xkcd" /></p>

<p>Language models are typically used in an <strong>autoregressive manner</strong>: which means it predicts the next word given context words, and it adds the predicted word to its context to predict the <em>next</em> word, and on and on. This is how all LMs work, be it N-grams or GPT.</p>

<p>The difference lies in <em>how</em> these models predict what the next word is. Transformers do this in a pretty complex process, at whos heart lies a mathematical attention mechanism, but we won’t go into that today.</p>

<p>N-grams, by contrast, predict the next word based on naive frequency of occurence. The word that <strong>occurs the most frequently</strong> with the previous \(N-1\) words, simply, is the predicted word!</p>

<p>This \(N-1\) is related to the \(N\) in N-grams:</p>
<ul>
  <li>A 1-gram (or unigram) predicts words randomly (0 context words)</li>
  <li>A 2-gram (or bigram) predicts the word that occurs the most after the last word (\(2-1=1\)) in the sequence</li>
  <li>A 3-gram (or trigram) predicts the word that occurs the most after the last two words (\(3-1=2\)) in the sequence</li>
</ul>

<p>And so on.</p>

<h3 id="a-touch-of-math">A Touch of Math</h3>
<p>Let’s consider a sequence of words in a sentence: \(w_1, w_2,..., w_{n-1}\) and our N-gram’s job is to predict what \(w_n\) is. Let us also define a <strong>vocabulary</strong> for our N-gram: the set of all possible words it can predict, denoted by \(V\).</p>

<p>The unigram case is pretty trivial, so let’s ignore it. Bigrams, however, are far more interesting. With what we’ve defined so far:</p>

<blockquote>
  <p>Bigrams predict a word \(w_n\) from \(V\) such that <br />
Count(\(w_{n-1}\), \(w_n\)) is maximum.</p>
</blockquote>

<p>All the <code class="language-plaintext highlighter-rouge">Count</code> function does is count the occurrences of words within it in that order.</p>


</article>
      </div>
    </main><!-- 100% privacy-first analytics -->
<script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
</body>
</html>