<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>N-grams: The dumbest language models</title>

  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="gautham | capemox" /><link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
  <link rel="stylesheet" href="/assets/css/main.css" />
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.css" integrity="sha384-OH8qNTHoMMVNVcKdKewlipV4SErXqccxxlg6HC9Cwjr5oZu2AdBej1TndeCirael" crossorigin="anonymous">

</head>
<body a="dark">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/">..</a><article>
  <p class="post-meta">
    <time datetime="2025-01-26 00:00:00 +0530">2025-01-26</time>
  </p>
  
  <h1>N-grams: The dumbest language models</h1>

  <p>N-grams are some of the simplest language models out there, and are pretty easy to understand (compared to LLMs at least). In this article, Iâ€™ll try giving you an inuitive understanding of N-gram LMs, while also explaining perplexity, an evaluation measure for such LMs, and also the inspiration for <a href="https://www.perplexity.ai">perplexity.ai</a>! Stay tuned for its release!</p>

<hr />

</article>
      </div>
    </main>
  </body>
</html>