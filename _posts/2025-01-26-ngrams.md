---
layout: post
title: "N-grams: The dumbest language models"
usemathjax: true
---

N-grams are some of the simplest language models out there, and are pretty easy to understand (compared to LLMs at least). In this article, I'll try giving you an inuitive understanding of N-gram LMs, while also explaining perplexity, an evaluation measure for such LMs, and also the inspiration for [perplexity.ai](https://www.perplexity.ai)! Stay tuned for its release!

---
<!-- ![n-gram-1](/media/n-grams/n_gram_1.webp) -->

## Back to Basics: Counting

Let's say you want to know what's the weather like tomorrow. You _can_ use state-of-the-art atmospheric models that solve numerous partial differential equations on big mainframe computers. But you can also, you know, just predict that it will rain tomorrow, because it rained today and yesterday.

This more or less highlights the difference between how LLMs (or the transformer model behind LLMs) work compared to N-grams. While the analogy isn't _exactly_ correct, it highlights just how simple N-grams are to grasp and build, and it also tells you that they're probably not very powerful.

![xkcd](https://imgs.xkcd.com/comics/10_day_forecast_2x.png)

Language models are typically used in an __autoregressive manner__: which means it predicts the next word given context words, and it adds the predicted word to its context to predict the _next_ word, and on and on. This is how all LMs work, be it N-grams or GPT.

The difference lies in _how_ these models predict what the next word is. Transformers do this in a pretty complex process, at whos heart lies a mathematical attention mechanism, but we won't go into that today. 

N-grams, by contrast, predict the next word based on naive frequency of occurence. The word that __occurs the most frequently__ with the previous $$N-1$$ words, simply, is the predicted word!

This $$N-1$$ is related to the $$N$$ in N-grams:
- A 1-gram (or unigram) predicts words randomly (0 context words)
- A 2-gram (or bigram) predicts the word that occurs the most after the last word ($$2-1=1$$) in the sequence
- A 3-gram (or trigram) predicts the word that occurs the most after the last two words ($$3-1=2$$) in the sequence

And so on.

### A Touch of Math
Let's consider a sequence of words in a sentence: $$w_1, w_2,..., w_{n-1}$$ and our N-gram's job is to predict what $$w_n$$ is. Let us also define a __vocabulary__ for our N-gram: the set of all possible words it can predict, denoted by $$V$$.

The unigram case is pretty trivial, so let's ignore it. Bigrams, however, are far more interesting. With what we've defined so far:

> Bigrams predict a word $$w_n$$ from $$V$$ such that \\
Count($$w_{n-1}$$, $$w_n$$) is maximum.

All the `Count` function does is count the occurrences of words within it in that order. 

